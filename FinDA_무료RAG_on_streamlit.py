import tempfile
import streamlit as st

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)

# --------------------------------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# --------------------------------------------------
st.set_page_config(page_title="100% Free Korean RAG", layout="wide")
st.title("ğŸ“„ 100% ë¬´ë£Œ í•œêµ­ì–´ RAG ì±—ë´‡ (Qwen + FAISS)")

# --------------------------------------------------
# ëª¨ë¸ ì„¤ì • (í•œê¸€ ì•ˆì • ì¡°í•©)
# --------------------------------------------------
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"

TOP_K = 1
MAX_CONTEXT_CHARS = 700
MAX_NEW_TOKENS = 256

# --------------------------------------------------
# ìºì‹œ: Embedding / LLM
# --------------------------------------------------
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name=EMBED_MODEL,
        encode_kwargs={"normalize_embeddings": True},
    )

@st.cache_resource
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

@st.cache_resource
def load_llm():
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, use_fast=True)

    model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL,
        torch_dtype=torch.float16,      # âœ… ë©”ëª¨ë¦¬ ì ˆê° í•µì‹¬
        low_cpu_mem_usage=True,         # âœ… ë¡œë”© ë©”ëª¨ë¦¬ ì ˆê°
        device_map="cpu",               # âœ… CloudëŠ” CPU
    )

    gen = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=128,             # âœ… 256 â†’ 128ë¡œ ë‚´ë¦¬ê¸°
        do_sample=False,
        temperature=0.0,
        repetition_penalty=1.05,
        return_full_text=False,
    )
    return gen, tokenizer

# --------------------------------------------------
# ìœ í‹¸
# --------------------------------------------------
def format_docs(docs, max_chars):
    text = "\n\n".join(d.page_content for d in docs if d.page_content)
    return text[:max_chars]

# --------------------------------------------------
# ì„¸ì…˜ ìƒíƒœ
# --------------------------------------------------
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# --------------------------------------------------
# PDF ì—…ë¡œë“œ
# --------------------------------------------------
uploaded = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

if uploaded:
    with st.spinner("PDF ë¶„ì„ ë° ì¸ë±ì‹± ì¤‘..."):
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded.getvalue())
            pdf_path = tmp.name

        loader = PDFPlumberLoader(pdf_path)
        docs = loader.load()

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=80
        )
        splits = splitter.split_documents(docs)

        embeddings = load_embeddings()
        st.session_state.vectorstore = FAISS.from_documents(splits, embeddings)

    st.success(f"ì¸ë±ì‹± ì™„ë£Œ (chunks: {len(splits)})")

# --------------------------------------------------
# ì±„íŒ… UI
# --------------------------------------------------
st.subheader("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ Q&A")

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if user_q:
    st.session_state.chat_history.append(
        {"role": "user", "content": user_q}
    )
    with st.chat_message("user"):
        st.write(user_q)

    if st.session_state.vectorstore is None:
        answer = "ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."
    else:
        retriever = st.session_state.vectorstore.as_retriever(
            search_kwargs={"k": TOP_K}
        )
        docs = retriever.invoke(user_q)
        context = format_docs(docs, MAX_CONTEXT_CHARS)

        gen, tok = load_llm()

        messages = [
            {
                "role": "system",
                "content": (
                    "ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ PDF ë¬¸ì„œì˜ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ "
                    "í•œêµ­ì–´ë¡œ ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
                    "ë¬¸ë§¥ì— ì—†ìœ¼ë©´ 'ë¬¸ë§¥ì— ê·¼ê±°í•´ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”."
                )
            },
            {
                "role": "user",
                "content": f"[ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{user_q}"
            }
        ]

        prompt = tok.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        out = gen(prompt)
        answer = out[0]["generated_text"].strip()

    with st.chat_message("assistant"):
        st.write(answer)

    st.session_state.chat_history.append(
        {"role": "assistant", "content": answer}
    )

